# RoboViz360
This repository presents a complete robotics project developed in ROS 2 Humble, where I designed and simulated a differential-drive mobile robot capable of obstacle detection and avoidance. The robot was modeled using URDF, integrated with a 2D LiDAR sensor, and visualized in RViz2.I implemented a custom Python navigation node using rclpy, which processes sensor data and publishes velocity commands directly to the robot. This project demonstrates strong fundamentals in robot modeling, sensor integration, and control logic within the ROS 2 ecosystem.

The development process began with building the robot model in URDF. The robot consists of two actuated wheels and a passive caster for balance, with the LiDAR sensor mounted on the base frame. A well-structured TF tree was defined to maintain transformations between odom, base_link, and laser_frame, ensuring accurate sensor readings and robot motion representation. The robot and its sensor were then launched in RViz2, which served as the visualization tool for both the robot and the surrounding obstacles. 

For the navigation component, I wrote a Python node that subscribes to LiDAR scan data on the /scan topic. The algorithm continuously checks the range readings, and whenever an obstacle is detected within a predefined threshold, the node immediately publishes a stop command to /cmd_vel. If the path is clear, the robot moves forward at a constant velocity. This reactive logic, though simple, ensures safe navigation and demonstrates how sensor feedback can be directly translated into robot motion without depending on prebuilt navigation frameworks.

To make the system reproducible and modular, I organized the repository with dedicated folders for URDF files, ROS 2 nodes, and launch files. The launch files bring up the robot_state_publisher, RViz2 with a pre configured environment, and the obstacle avoidance node in a single command, ensuring a smooth execution workflow. The entire pipeline can thus be run seamlessly, from robot description loading to real-time obstacle avoidance.

Finally, the project was tested by adding obstacles in RViz at different distances and observing the robot’s behavior in real time. The LiDAR readings were visualized as point clouds, while velocity commands were monitored to verify that the robot stopped precisely in front of obstacles. The demo showcases the robot’s ability to move autonomously and react instantly to changes in its environment, highlighting the effectiveness of the approach.

This project was developed on my personal i3 laptop, which did not support Gazebo simulations efficiently. As a result, I adapted the workflow to use RViz exclusively for visualization. This choice reflects my ability to optimize resources and adapt to hardware constraints while still delivering a fully functional robotics simulation project.
